{
  "riskMap": [
    {
      "riskCategory": "Behavioral Risks",
      "risk": "Accuracy",
      "riskDescription": "All AI models, including generative AI (GenAI) models, can experience suboptimal performance in certain scenarios, and the unknown reason for this nonperformance can be alarming for other stakeholders. While occasional performance dips may be tolerable, the stakes are higher as GenAI becomes increasingly widespread and is deployed more deeply within the organization.\r\nThe risk of factual inaccuracies is often referred to as “hallucinations,\" but the issue extends beyond that. GenAI models are also prone to providing false or confusing responses, a phenomenon not entirely new to the field of AI. These challenges are historically known as misclassifications or overgeneralizations. Further, GenAI tools are misleading with their confident tone they often take.",
      "initialLikelihood": 4,
      "initialImpact": 5,
      "initialRiskLevel": 20,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Informing Users: Raising awareness among users about the limitations of GenAI models is crucial. As is encouraging users to maintain a critical approach when interacting with GenAI outputs, and verifying information through trusted sources before making decisions based on AI-generated content.\n\nTesting and Prototyping: To ensure safe deployment, rigorous testing is required. This includes prototyping and pilot projects, which lead to an initial understanding of the magnitude of the problem. How will the chosen GenAI model perform in real-world scenarios? This involves:\r\nValidation: Comparing model outputs against known truths or expert opinions to assess accuracy.\r\nA/B Testing: Experimenting with different models or model versions to identify which performs best for your specific use cases.\r\nContinuous Evaluation: Regularly assessing the model's performance over time to catch and correct drifts in accuracy or relevance.\n\nAdvanced Querying Techniques: Crafting prompts that are clear, specific, and contextually rich can significantly enhance the quality and accuracy of GenAI responses. Training users and developers in effective prompt engineering can lead to better outcomes and reduce the incidence of hallucinations. Also, asking the same question in different ways can not only help in identifying inconsistencies in responses but also in triangulating the most accurate information. This approach can reveal the model's confidence and consistency across various formulations of a question.\n\nChoosing Appropriate LLMs: Not all LLMs are created equal; they may have been trained on different datasets, optimized for different tasks, or updated at different times. Selecting the right model for the specific need can significantly affect the accuracy and relevance of the output. Assessing and choosing vendors based on the performance and reliability of their models in relevant domains is crucial. This might involve benchmarking exercises or pilots to compare models on specific tasks.\n\nContinuous Feedback Loops: Establish mechanisms for users to report inaccuracies or \"hallucinations\" experienced during their interactions with GenAI systems. This feedback can be invaluable for training and improving models.",
      "agreedMitigation": "Implement rigorous model validation and require human verification of GenAI outputs to ensure accuracy meets regulatory standards. 8090 and Dompe have aligned on pre-agreed upon metrics for success and accuracy for each AI project. See https://dompe.sharepoint.com/:w:/s/MedicalAffairsSharedDrive/EeIHdhOFnLVHhW1PSfgqwAMBy2Vk8hydro21t0aCNTNTyQ?e=v32ebp \r\n \r\n**8090's Responsibilities:**\r\n1. **System Design & Validation (RAG):** 8090 will implement and validate a Retrieval-Augmented Generation (RAG) architecture. This system will use a validated, version-controlled corpus of Dompé's GxP documents as the single source of truth. (GxP refers to regulations for life sciences organizations). The system will be designed to prevent access to unapproved external sources for GxP-related queries, aligning with 21 CFR 11.10(a) and EU Annex 11.3. \r\n\r\n2. **Prompt Engineering & Guardrails:** 8090 will develop and validate standardized prompt templates that instruct the model to use only the provided context and to state \"Information not found in provided context\" when an answer is not present.\r\n\r\n**Dompe's Responsibilities:**\r\n3. **Human-in-the-Loop (HITL) Verification:** Dompe will ensure that all AI-generated outputs for GxP-regulated decisions are verified and approved by a qualified expert. This process, including electronic signatures, must comply with 21 CFR Part 11 and EU Annex 11.13.\r\n4. **Continuous Monitoring & CAPA:** Dompe will implement a user feedback mechanism for reporting inaccuracies. These reports will trigger Dompe's formal CAPA process.",
      "proposedOversightOwnership": "HR, Compliance",
      "proposedSupport": "LOBs, Legal, IT Security, AI Leader",
      "notes": "As this is a corporatewide policy/training issue",
      "residualLikelihood": 2,
      "residualImpact": 3,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "Behavioral Risks",
      "risk": "Biased Outputs",
      "riskDescription": "Generative AI models, particularly Large Language Models (LLMs), are susceptible to producing biased outputs. This is primarily because these models are trained on vast datasets compiled from the internet and many other sources, which often contain biased, inaccurate, or otherwise problematic information. The inherent bias in these datasets reflects the biases present in the broader world, leading to AI systems that can inadvertently perpetuate or even amplify these biases in their outputs. Mitigation Tactics are below.",
      "initialLikelihood": 3,
      "initialImpact": 4,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Informing Users: Provide notice to users of algorithmic, model or data bias.\n\nTesting: Rigorously test both developed and deployed technology solutions to understand if bias is applicable. For information on representative vendors and tools please refer to Innovation Guide for Generative AI in Trust, Risk and Security Management\n\nChoosing Appropriate GenAI solutions: Specifically review training data in internal solutions or ask vendors how models are trained\n\nContinuous Feedback Loops: Establish a similar feedback loop that pertains to Bias incidents.",
      "agreedMitigation": "8090 will assist Dompe in identifying potential sources of statistical bias in the document set. Perform bias audits on AI outputs and use diverse, representative data sets; enable human review to correct any biased content before use. Use bias detection tools and grounding mechanisms to check and correct AI model outputs from known biases. The system will allow Dompe to add a disclaimer to AI-generated content, noting the potential for bias and the need for critical review. \r\n\r\nProcedural Controls (Owned by Dompe): 1. **Prohibited Use Policy:** Dompe will define and enforce a policy prohibiting the AI's use for high-risk HR decisions (e.g., hiring, promotion) as defined by the EU AI Act. 2. **Diverse Human Review:** Dompe will establish a process for reviewers to audit AI outputs for subtle or contextual bias before use in sensitive applications.\r\n\r\nBias Mitigation and Fairness Framework: 1. Support for Prohibited Use Case Policy: 8090 will provide controls to help Dompe enforce its policy prohibiting the use of the GenAI system for automated decision-making in high-risk HR activities (as defined by EU AI Act Annex III.21). 2. Data Governance for RAG: If a RAG architecture is used, the system will support Dompe's data governance procedures for reviewing the internal document corpus for bias. 3. Fairness Testing & Auditing: 8090 will provide tools to enable Dompe to conduct periodic fairness testing and auditing for biased outputs. This aligns with the EU AI Act's requirements for HRAIs.6 4. Transparency and Disclosure: The system will allow Dompe to include a clear disclaimer noting the potential for algorithmic bias. This aligns with EMA guidance on transparency.",
      "proposedOversightOwnership": "HR, Compliance",
      "proposedSupport": "AI Leader, IT, Privacy",
      "notes": "This can be part of a acceptable use policy.",
      "residualLikelihood": 2,
      "residualImpact": 3,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "Behavioral Risks",
      "risk": "Outdated Information",
      "riskDescription": "All LLMs and most GenAI models will have “knowledge” cutoffs. Even if the update cycles will get shorter (from several months currently to maybe weeks or days), keeping LLMs and other GenAI updated will remain a challenge for the foreseeable future. As an example: The recently announced ChatGPT 4.0 version has a knowledge cutoff point from September 2021 and it is not so clearly tagged. One has to ask specifically what the current knowledge cutoff is. While the \"older\" ChatGPT 4.0 version has a cutoff point of December 2023.",
      "initialLikelihood": 3,
      "initialImpact": 4,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Clearly Flag Knowledge Cutoffs: Continuously advising of knowledge cutoff dates will help users know which questions to ask and which answers likely not to trust. This must be part of policy and staff training.",
      "agreedMitigation": "Use AI models that allow frequent updates or real-time data retrieval. Timestamp AI outputs and require users to verify against current sources. Ground the AI with up-to-date databases or web searches. Train users to double-check AI-provided facts against the latest literature.\r\n\r\n**8090's Responsibilities:**\r\n1. **Enforce RAG-Only Responses:** The system will be configured with strict context grounding to rely exclusively on the provided, version-controlled documents for GxP queries, with the base model's knowledge disabled for these use cases.\r\n2. **Document Version Control:** The RAG knowledge base will be integrated with Dompe's document management system to use only the latest, approved document versions.\r\n3. **Source Citation:** All AI-generated responses will include a precise citation (document name, version, and page/section number) for traceability and verification.",
      "proposedOversightOwnership": "IT Leader, AI Leader",
      "proposedSupport": "LOB",
      "notes": "As this task is quite simplistic, ownership is more flexible.",
      "residualLikelihood": 1,
      "residualImpact": 2,
      "residualRiskLevel": 2
    },
    {
      "riskCategory": "Behavioral Risks",
      "risk": "Scope Violations",
      "riskDescription": "Customized GenAI models are supposed to be constrained in their application scope. But actually no GenAI model will really be “aware” of its intended scope. Constraining GenAI models to fit a particular scope is inherently difficult, especially if the underlying foundation model has been trained on a much wider training set. There are no guarantees that scope constraints based on “refinement” will hold. Example: It is conceivable that a bank could create a customer-facing chatbot - and when being asked for alternative products could cite competitors products.",
      "initialLikelihood": 4,
      "initialImpact": 3,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Employee Training and Communications: to inform employees about this problem.\n\nImplement Application Red Teaming: Attempts to uncover a library of examples of such scope violations. This library can prove extremely useful for updating a customized GenAI model and avoid such scope violations in the future.\n\nAdditional Prompt Engineering: can guide the scope into a better direction and thus avoid most scope violations. This can implemented as part of a RAG architecture (see RAG introduction).\n\nConsider Choosing Different Foundation models: which might be less prone to such scope violations..",
      "agreedMitigation": "Define strict use-case boundaries and implement prompt filters; educate users to avoid queries outside the AI’s domain and to flag any off-topic responses.\r\n\r\n1. Technical: Implement and validate standardized prompt templates that explicitly instruct the model to only use the provided document context and to refuse to answer if the information is not present.\r\n\r\n2. Procedural: Conduct adversarial testing (\"Red Teaming\") to actively find prompts that cause scope violations and use these findings to refine prompt guardrails.\r\n\r\n3. Compliance: The system's intended use and scope must be clearly defined in the validation plan and user training materials, as required by GxP principles.",
      "proposedOversightOwnership": "Compliance, HR",
      "proposedSupport": "AI Leader, IT, Privacy",
      "notes": "This can be part of a acceptable use policy.",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "Behavioral Risks",
      "risk": "Availability",
      "riskDescription": "Generative AI (GenAI) models, particularly those with billions of parameters, can experience challenges with response times, especially under the strain of multiple concurrent users. This issue is known also as a scalability risk and is exacerbated by the inherent complexity and size of these models. Models of this size and scope require significant computational resources to process inputs and generate outputs efficiently.",
      "initialLikelihood": 4,
      "initialImpact": 4,
      "initialRiskLevel": 16,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Infrastructure Provisioning: to provide GenAI models with more efficient infrastructure.\n\nModel Optimization: to utilize simpler and more efficient GenAI solutions.\n\nQuery Management: to constrain the number of query by user group.",
      "agreedMitigation": "Mitigate availability risk by selecting a vendor with robust SLAs and implementing a business continuity plan that includes failover procedures and manual workarounds. Establish backups: Maintain an alternative process (or LLM provider) for document access when the AI is unavailable and test disaster recovery procedures. Implement engineering practices to reduce LLM spend while sustaining high levels of accuracy.\r\n\r\n[A] System & Contractual Controls (Provided by 8090): **High-Availability Architecture:** The solution will be deployed across multiple physical locations (AWS Availability Zones) to prevent single points of failure. The choice of how many AZs and regions depends on the use case service availability objectives and the individual workloads.  For instance, due to hardware capacity limitations, 8090 typically enables multi-region failover for Amazon Bedrock as hosted model provider. Baseline Uptime commitments for all 8090 services will be 99.5%.\r\n\r\n[B] Procedural Controls (Owned by Dompe):**. **Business Continuity Plan (BCP):** Dompe will maintain and test a BCP outlining manual procedures for essential tasks if the system is unavailable, as required by GxP regulations (EU Annex 11).\r\n\r\n[C] Compliance Controls (Owned by Dompe):**. The 8090 contract includes a Service Level Agreement (SLA) with uptime commitments and penalties, transferring some financial risk.,including system downtime contingencies in Part 11-compliant validation documentation.",
      "proposedOversightOwnership": "IT Leader",
      "proposedSupport": "external Cloud / GenAI Service provider",
      "notes": "This is a core capability of the IT department",
      "residualLikelihood": 3,
      "residualImpact": 2,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "Behavioral Risks",
      "risk": "Limited Vendor Liabilities",
      "riskDescription": "Most GenAI vendors will try to make clear that users own the output they create and any liability associated with it (see for example “Terms of Use” of OpenAI and others).",
      "initialLikelihood": 3,
      "initialImpact": 3,
      "initialRiskLevel": 9,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "Comprehensive Review of Vendor Contracts: Facilitate and support a thorough review of the terms and conditions, privacy policies, and service level agreements (SLAs) of GenAI vendors. This review should aim to understand the scope of liability and any responsibilities that the vendor disclaims. Legal counsel can help negotiate terms that might offer better protection for your organization.\n\nLegal and Compliance Training: In addition to policy and general user training, provide specific legal and compliance training for employees who manage contracts with GenAI vendors or who are responsible for integrating GenAI outputs into business processes. This training should cover the implications of vendor terms and how to mitigate risks associated with these terms.\n\nRisk Assessment and Management Plans: Develop a risk management plan specifically addressing the risks associated with GenAI outputs. This plan should include assessing the potential impact of incorrect, biased, or inappropriate outputs on the business and devising strategies to mitigate these risks.\n\nEnd Users must remain on alert: Report strange outputs of GenAI systems, because end-user organizations will most likely be ultimately liable.",
      "agreedMitigation": "Establish clear contractual terms regarding data usage, retention, and intellectual property protection, with appropriate limitations on use of customer data for model training\t\r\n\r\n- Technical: Perform independent validation of vendor's AI outputs rather than relying solely on vendor guarantees. Implement systematic testing protocols to verify accuracy and reliability of AI-generated content before use in GxP processes. \r\n- Procedural: Legal and procurement teams must conduct a thorough review of all vendor terms, service level agreements, and privacy policies before any contract is signed. Institute regular vendor audits with documented findings and include specific liability clauses for quality issues in all agreements. Establish internal checks and validation processes to catch errors independently.\r\n- Compliance: Negotiate for enterprise-grade contracts that include clauses for IP indemnification and warranties regarding the system's performance and compliance with applicable laws. Only engage vendors that agree to meet pharmaceutical compliance requirements including audit trails, quality assurance protocols, and 21 CFR Part 11/EU Annex 11 standards, even if they attempt to limit liability. Obtain appropriate insurance coverage to supplement vendor indemnities and cover potential AI failures.",
      "proposedOversightOwnership": "Legal, Compliance",
      "proposedSupport": "AI Leader, Service Provider, LOB",
      "notes": "This is a legal responsibility",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "Transparency Risks",
      "risk": "Lack of Explainability",
      "riskDescription": "Unlike traditional AI, Explainable AI approaches are not easily applicable to GenAI models, because of their immense parameter complexity and the extreme opaqueness of the deep learning architecture. Also many AI approaches have been developed for discriminative AI approaches and not for generative approaches.",
      "initialLikelihood": 4,
      "initialImpact": 3,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Thorough Testing: will take over trust-creation of how GenAI systems work, with their growing complexity.\n\nPrefer Standard AI over GenAI, because standard AI has much better explainability methods.\n\nChain-of-Thought Prompting: is an iterative prompting strategy able to deliver step-by-step reasoning serving explainability requirements.",
      "agreedMitigation": "Accept the opacity of the core model but reduce the practical risk by implementing a system that provides full traceability and transparency for the inputs and outputs, shifting the focus from explaining the \"how\" to verifying the \"what\". \r\n- Technical: The RAG architecture with precise source citation provides traceability, showing which validated document was used to generate an answer, even if the synthesis process is opaque. Use model variants or add-ons that highlight which data influenced an output;\r\n- Procedural: All human verifications of AI outputs must be logged, creating an audit trail of accountability as required by 21 CFR 11.10(e). \r\n- Compliance: Update SOPs to ensure any AI-derived conclusion is independently verified and justified in records. This approach aligns with the EU AI Act's and EMA's focus on transparency and human oversight as the primary safeguards for complex models.",
      "proposedOversightOwnership": "AI Leader",
      "proposedSupport": "IT Leader",
      "notes": "This is a core capability of an AI Team or the Data Science / Analytics function",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "Transparency Risks",
      "risk": "Failure to Disclose AI",
      "riskDescription": "Legislation in various jurisdictions now mandates the explicit disclosure of AI involvement in interactions with consumer - this is commonly known as provenance requirements. Notable examples include the California chatbot law, which requires organizations to clearly inform consumers when they are communicating with a bot. Similarly, forthcoming regulations, such as the European AI Act, aim to ensure transparency when AI systems play a role in business transactions or decision-making processes.",
      "initialLikelihood": 3,
      "initialImpact": 3,
      "initialRiskLevel": 9,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "Training and Policies: AI leaders should work with their Compliance / HR partner so that clear organizational policies are put into practice that mandate the disclosure of AI involvement in user interactions.\n\nAffirm Disclosure Mechanisms: AI Leaders should affirm that their development partners embed mechanisms within AI systems that provide clear disclosures about AI involvement.\n\nRegular Compliance Audits: AI leaders will again work with Audit & Compliance staff to ensure that AI disclosure practices are in compliance with both internal policies and relevant laws. These audits can help identify gaps in compliance and areas for improvement.",
      "agreedMitigation": "Establish and enforce a comprehensive organizational policy governing AI-generated content handling, labeling, and transparency to ensure regulatory compliance.\r\n- Technical: Implement watermarking and metadata tagging for all AI-generated content to ensure its origin is permanently identifiable. Include clear \"AI-generated\" labels embedded in document properties and visible markers where appropriate for all AI-produced materials.\r\n- Procedural: Update corporate acceptable use policy to mandate explicit labeling of any AI-generated content shared externally. Revise informed consent forms and patient communications to disclose when AI tools are used in document creation or decision-making processes. Provide specific training to ensure all staff understand and follow labeling requirements.\r\n- Compliance: Adhere to Article 52 of the EU AI Act, which mandates disclosure for AI-generated content. Maintain comprehensive records documenting where AI assists in decision processes to meet transparency obligations. Ensure all labeling practices align with pending regulatory requirements while supporting audit trails required by 21 CFR Part 11 and EU Annex 11.",
      "proposedOversightOwnership": "Compliance, HR",
      "proposedSupport": "AI and IT Leader",
      "notes": "This can be part of a acceptable use policy.",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "Security and Data Risks",
      "risk": "Sensitive Information Leakage",
      "riskDescription": "The risk is that information entered in a publicly hosted GenAI may become part of the vendor's training set. And there is a risk of data breaches or unauthorized access when such information is transmitted over the internet. This was already a problem with machine translation apps and proofreading apps hosted on the web (e.g. Google Translate and DeepL). And it became even more profound with LLM-based applications such as OpenAI’s ChatGPT, Google Gemini (formerly known as Bard) and others.",
      "initialLikelihood": 5,
      "initialImpact": 5,
      "initialRiskLevel": 25,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Include in Acceptable Use Policy: Explain to users that the input of sensitive information to GenAI systems is not allowed and will be monitored.\n\nDevelop a role-based application use approval workflow: Add embedded controls that require review and approval of AI applications prior to deployment. Reviewers should include required personnel from D&A, IT Security, Privacy and Legal based on the risk of the specific use case for the application.\n\nConstruct Training: Explain to users what sensitive information is (e.g. proprietary, copyrighted, IP protected or confidential information) and how it may be a danger to input this into GenAI applications without care.\n\nDeploy GenAI Content Anomaly Detection and AI Application Security tools that enforce enterprise AI policies for acceptable use and prevent data leakage (see G00776480) due to benign mistakes or malicious interventions and attacks (see TRiSM Innovation Guide).\n\nPrefer privately hosted / on-prem GenAI solutions: A simple mitigation against this risk is to offer enterprise users privately hosted versions of GenAI applications.\n\nVendor Assessments: Perform due diligence on GenAI service providers to understand their data handling and privacy practices, particularly how they manage and use data entered into their systems.",
      "agreedMitigation": "Zero-Trust Data Protection Architecture: 1. Private Hosting & Contractual Guarantees: The GenAI system must be deployed in a private, segregated cloud environment (e.g., a dedicated VPC/project).  Dompé's data (prompts, documents, outputs) will not be used to train vendor models. 2. Data Loss Prevention (DLP) at Ingress/Egress: Implement a data-centric security tool to scan all prompts and responses in real-time. Policies must be configured to detect and redact/block specific PII, PHI, and sensitive IP keywords from being sent to or returned by the model. 3. Robust Access Control & Logging: Enforce strict, role-based access controls based on the principle of least privilege (NIST AC-6). All user interactions, including every prompt and response, must be logged to a secure, immutable audit trail (21 CFR 11.10(e)) to support incident investigation. 4. Encryption: All data, both in the RAG document store and in transit, must be encrypted using strong, validated cryptographic modules (e.g., AES-256). Dompé should use enterprise-grade encryption standards to control data access fully. This aligns with HIPAA technical safeguards [§ 164.312(a)(2)(iv)]. 5. Zero Data Retention Policy: 8090 (vendor) will follow a zero-data-retention policy, which means 8090 will not store any of Dompé's proprietary data or IP in short term or long term storage. \r\n\r\nApply data minimization: anonymize or pseudonymize data before using it with GenAI whenever possible. Get explicit consent for any new use of personal data with AI and involve the Data Protection Officer in approving such use cases. Additionally, ensure any AI provider handling our sensitive data signs a robust Data Processing Agreement/BAA. Use synthetic or anonymized data sets for AI model training to avoid using real personal data. Update privacy notices to transparently inform data subjects of any AI-based processing (fulfilling GDPR transparency duties), and allow opt-outs where required; Implement a review step under GDPR Article 35 (DPIA – Data Protection Impact Assessment) for any GenAI use of sensitive information, to formally evaluate and mitigate privacy risks before launch.",
      "proposedOversightOwnership": "HR, Compliance",
      "proposedSupport": "IT",
      "notes": "The communication part is HR / Compliance responsibility. The monitoring part is an IT competency.",
      "residualLikelihood": 2,
      "residualImpact": 4,
      "residualRiskLevel": 8
    },
    {
      "riskCategory": "Security and Data Risks",
      "risk": "Copyright Infringements",
      "riskDescription": "GenAI vendors are currently under scrutiny and even legal investigation as they might have used copyrighted materials as training data. This may be subject to legal consequences. In the future we expect this risk category to disappear entirely, as GenAI vendors are quickly moving to content usage syndicates with major content providers.",
      "initialLikelihood": 3,
      "initialImpact": 3,
      "initialRiskLevel": 9,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "IP Indemnification Clauses: Identify vendors and specific products that offer intellectual property (IP) indemnification clauses (e.g. OpenAI GPT4.0 does not have an indemnification provision but GPT4.0 enterprise does).\n\nDue Diligence and Vendor Assessment: Before engaging with a GenAI vendor, conduct thorough due diligence to assess how they source their training data. ensure that contracts with GenAI vendors include specific warranties stating that the vendor has the right to use the data for training their models and that the use of the GenAI model by your enterprise will not infringe on third-party copyrights.\n\nCopyright Compliance Tools: Utilize copyright compliance tools and services to monitor the outputs generated by the GenAI model for potential copyright infringements. Here this can be accomplished with Content Anomaly Detection tools that monitor output risks including potential copyright violations (see TRISM Innovation Guide).\n\nLicensing Agreements for Protected Materials: If anything of the above cannot be implemented sufficiently, and if your enterprise's use of GenAI involves generating outputs that may be close to copyrighted material, consider obtaining licenses for the use of such materials.\n\nContingency Planning: In addition to having a second vendor as a backup, develop a comprehensive contingency plan that outlines the steps to be taken in case of a need to switch vendors.\n\nTraceability of Outputs: Logging of generated outputs, or sessions, can be one means to track outputs, in case of copyright infringement being raised later. See AI TRiSM research: Content Anomaly Detection tools that monitor and record generated outputs, and identify outliers including potential copyright infringements.",
      "agreedMitigation": "Use plagiarism detection on AI outputs and limit use of GenAI for tasks prone to copying. Require human review of any AI-generated content for originality before publication. Integrate content filters or checkers to flag verbatim text that matches external sources; Establish guidelines that AI outputs must be treated as drafts and thoroughly edited/cited by staff. \r\n\r\nIP Indemnification and Output Management: \r\n\r\n 1. Vendor IP Indemnification: Select an enterprise-grade GenAI vendor that provides a contractual IP indemnification clause, protecting Dompé from copyright claims arising from the use of the model's output. \r\n2. Prohibited Use for Public Content: The Acceptable Use Policy must prohibit the use of the GenAI system for creating original content intended for public publication or marketing materials. Its use should be restricted to internal analysis and summarization. \r\n3. Traceability of Outputs: All generated outputs must be logged and linked to the input prompt and the user who initiated the request. This provides a clear audit trail in the event of a future claim, aligning with GxP traceability requirements.\r\n4. Zero Data Retention Policy: 8090 will follow a zero-retention policy, which means 8090 will not store any of Dompé's proprietary data or IP.",
      "proposedOversightOwnership": "Legal, Compliance",
      "proposedSupport": "IT Security, AI Leader, Lob",
      "notes": "This is a shared responsibility.",
      "residualLikelihood": 1,
      "residualImpact": 3,
      "residualRiskLevel": 3
    },
    {
      "riskCategory": "Security and Data Risks",
      "risk": "Hackers Abuse In-House GenAI Solutions",
      "riskDescription": "Adversarial Prompting Attacks: Such as Direct Prompt Injection (DPI) and Indirect Prompt Injection (IPI), where attackers manipulate AI responses through crafted inputs.\r\nModel query Attacks: Where attackers infer sensitive information from the model's outputs. This also includes “inversion attacks” (get training data from outputs).\r\nData poisoning: Applies to training data, but also to RAG data. Attackers could plan malicious data in sources used by the GenAI application: instructions for the LLM, malicious URL links and other types of inaccurate content..\r\nVector Database Attacks: Targeting the databases that store the vectors used by AI models for making predictions or generating content.\r\nOrchestration layer vulnerabilities: The application code built to instrument LLMs might include vulnerabilities. This includes API calls to third-party models, but also third-party components, which could have known vulnerabilities or be poisoned.",
      "initialLikelihood": 4,
      "initialImpact": 5,
      "initialRiskLevel": 20,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Ensure secure development life cycle practices are implemented. This includes data security, application security and software supply chain practices.\n\nImplement API security: Ensure good API authentication management practices and monitor for abnormal API calls.\n\nMit4x: Inventory and managed vulnerabilities of third-party components: Work with other teams to monitor vulnerabilities disclosed for the third-party libraries, AI framework and model instrumentation tools\n\nRegularly test the application: Work with the application security testing team to ensure smooth automation of the application testing process, during development, but consider also dynamic application security testing.\n\nEvaluate embedded security components: A growing number of AI security tools are available. Many of them require to be directly integrated in the application, or at least to be deployed inline the workflow of the AI application.",
      "agreedMitigation": "Enforce strong access controls (unique logins, MFA) and test the AI with adversarial prompts to improve its resistance. Monitor AI usage logs for anomalies and limit the model’s access to only necessary data. Technical: apply NIST 800-53 AC controls to ensure only authorized use and implement prompt injection defenses (input sanitization, response monitoring); Procedural: include GenAI in regular IT security audits and incident response drills; Compliance: ensure the AI system meets SOC 2 security criteria, and document these controls in risk assessments.\r\n\r\nDefense-in-Depth Security Architecture:  \r\n 1. Secure Application Gateway: All traffic to the GenAI application must pass through a web application firewall to protect against common web exploits and DDoS attacks.API access must be secured with strong authentication and rate limiting.\r\n 2. Input/Output Sanitization & Validation: Implement strict input validation to detect and neutralize prompt injection attempts. Use text embedding similarity checks to compare new prompts against a library of known malicious prompts. \r\n 3. All outputs must be scanned for harmful content or data leakage before being displayed to the user.\r\n 4. Secure RAG Data Pipeline: The process for ingesting documents into the RAG knowledge base must be secured. Documents must come from a trusted, access-controlled source. Implement integrity checks (e.g., checksums) to detect unauthorized modifications (Data Poisoning). This is a critical SI (System and Information Integrity) control under NIST 800-53.28 \r\n 5. Infrastructure Isolation: The entire backend infrastructure (application servers, model endpoints, vector database) must be contained within a secure VPC Service Controls perimeter to prevent data exfiltration and unauthorized access from outside the trusted boundary.   6. Red Teaming: Conduct periodic adversarial testing (Red Teaming) specifically designed to attack the GenAI system, attempting prompt injection, data exfiltration, and other AI-specific attacks.",
      "proposedOversightOwnership": "IT Security",
      "proposedSupport": "",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 4,
      "residualRiskLevel": 8
    },
    {
      "riskCategory": "Security and Data Risks",
      "risk": "Unauthorized Information Access via LLMs",
      "riskDescription": "Scenario in which an internally refined LLM was trained on an internal corpus of documents. This corpus was subject to certain user access rights. Now it becomes critical that employees with lesser user access rights get access to this chatbot based on this LLM.",
      "initialLikelihood": 4,
      "initialImpact": 3,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Internal LLMs must comply with Clearance Levels: Create different versions of LLMs for different users based on their clearance level. This ensures that only those authorized to see the training corpus also will have access to the LLMs itself.\n\nUtilize Privacy Enhanced Training Data: Potentially privacy-enhancing strategies might also help to create informative LLMs where the sensitive part of the documents are masked or obfuscated.",
      "agreedMitigation": "Access to prompts is controlled via MFA/unique logins and is only accessible by 8090. Any user interaction with LLM inference is controlled through a unique login and scoped RBAC.\r\n\r\nTechnical: Integrate the RAG system's retrieval mechanism with Dompé's existing Active Directory or identity management system. Before retrieving any document chunk, the system must verify the user's access rights to the source document in real time.\r\n\r\nProcedural: Document access control rules and policies clearly within the system's operational SOPs. Require managerial approval for any broad data access requests by AI systems. Conduct quarterly penetration tests using limited-privilege accounts to verify that access controls are functioning correctly. Review access logs regularly to identify anomalies or unauthorized access attempts.\r\n\r\nCompliance: Apply the principle of least privilege as required by NIST 800-53 (AC family) and SOC 2 security frameworks. Have the Privacy Office conduct regular audits of the AI's data access patterns to ensure no policy violations. Maintain audit trails that demonstrate compliance with data minimization requirements under GDPR and support forensic analysis if needed.",
      "proposedOversightOwnership": "IT Security, Compliance, Privacy",
      "proposedSupport": "--",
      "notes": "Can be a critical task to be executed by either one of those 3 teams, while the others can approve / assist.",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "Other Risks",
      "risk": "Failing to Keep Up With Regulations",
      "riskDescription": "The regulatory environment for AI is complex and dynamic, encompassing a wide range of existing and forthcoming regulations that address data privacy, security, equality, consumer protection, transparency and more. Examples include the EU AI Act, General Data Protection Regulation (GDPR), NIS2 standard, the Equality Act 2010, and the California Consumer Privacy Act (CCPA), among others. Compliance demands can vary significantly across different industries and geographical regions.",
      "initialLikelihood": 4,
      "initialImpact": 4,
      "initialRiskLevel": 16,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Designate Responsibility: Assigning clear accountability for regulatory compliance within the organization is crucial.\n\nRegulatory Scanning and Analysis: Establish processes for continuously scanning the regulatory landscape for new developments and documenting them in a central repository.\n\nFramework Development: Develop a robust compliance framework tailored to the most comprehensive GenAI legislation (currently the EU AI Act), incorporating risk assessment methodologies, compliance controls, and governance structures to ensure adherence to all applicable laws and regulations.\n\nContinuous Education and Training: Implement ongoing education programs for staff to keep abreast of new regulations and understand their implications for GenAI applications. This includes training for developers, data scientists, legal teams, and decision-makers.",
      "agreedMitigation": "Proactive Regulatory Intelligence Program: 1. Designate AI Regulatory Lead: Assign clear responsibility to a senior individual or group (e.g., within the Legal or Compliance department) for monitoring the global AI regulatory landscape. 2. Establish Regulatory Scanning Process: Implement a formal process for continuously scanning for new regulations, draft guidance, and enforcement actions from key bodies (EMA, FDA, EU Commission, etc.). This should be documented in a central repository. 3. Develop an AI Compliance Framework: Create and maintain a compliance framework that maps Dompé's AI policies and controls to the requirements of key regulations like the EU AI Act. This framework should be reviewed and updated at least annually or upon the release of significant new legislation. 4. Cross-Functional Training: Conduct regular training for key stakeholders (Legal, IT, QA, Data Science) on the implications of new AI regulations for their roles and responsibilities.",
      "proposedOversightOwnership": "Legal, Compliance, Privacy",
      "proposedSupport": "AI Leader, IT security, Privacy, LOB",
      "notes": "Whoever owns regulations should be primarily responsible.",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "Other Risks",
      "risk": "Employees Abusing GenAI",
      "riskDescription": "Employees (esp. software engineers) are using generative AI to support work regardless of whether their employers permit use. This abuse could be voluntary, where the employee makes a choice to use AI regardless of policy guidance, or involuntary, where the employee may not know what they are doing poses a risk to the organization. There are hundreds of scenarios conceivable. For example:\r\na> Customer Service employees may require a sentiment detection system for analyzing customer emails. Instead of going through appropriate IT / application governance procedures, they may simply task an LLM for suggesting whether a customer email bears positive, neutral or negative sentiment. They may even ask the LLM to classify the email into certain task categories for routing.\r\nb> An HR employee might use an LLM system to classify resumes against job profiles.\r\nc> Any administrative employer could use LLMs extensively to do their job without notifying anyone.\r\nThis is problematic for several reasons:\r\n1> If this is a publicly hosted LLM this presents significant violations to data privacy.\r\n2> Regardless of the hosting strategy the biggest risk is verification of the results. This may lead to significant reliability risks.",
      "initialLikelihood": 4,
      "initialImpact": 4,
      "initialRiskLevel": 16,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Build a Governance Structure: Every systematic / programmatic use of GenAI systems must be documented and reported and ultimately approved by a central office.\n\nTraining and Communication: Employees, esp. their managers must be constantly reminded that if they use LLMs that they are accountable for the outputs that LLMs produce.\n\nAccess Controls and Usage Monitoring: Implement strict access controls to GenAI systems and monitor usage to ensure compliance with policies.",
      "agreedMitigation": "Mitigate the risk of unapproved GenAI use by providing a sanctioned, secure, and effective internal tool while implementing technical controls and continuous user education to discourage and detect unauthorized external services. Implement a strict Acceptable Use Policy for GenAI and train all staff. Monitor AI usage for red flags (attempts at forbidden queries) and set up an incident protocol for any abuse.\r\n\r\n- Technical: Enable logging and perhaps content filtering on the AI platform to catch policy violations. Deploy network-level controls and CASB (Cloud Access Security Broker) policies to block or monitor access to known public GenAI websites from corporate networks and devices.\r\n-Procedural: Users should be required to sign an AI usage agreement and undergo training on ethical AI use and acceptable use policy.\r\n- Compliance: Enforce sanctions for misuse and review incidents in compliance committees to reinforce their seriousness. Providing secure internal tools and clear governance aligns with the expectation of proactive risk management in frameworks like NIST and the EU AI Act. ",
      "proposedOversightOwnership": "IT Leader, Compliance",
      "proposedSupport": "Executive Management, LOBs",
      "notes": "Responsibility may vary by organization.",
      "residualLikelihood": 2,
      "residualImpact": 4,
      "residualRiskLevel": 8
    },
    {
      "riskCategory": "Other Risks",
      "risk": "Sustainability/Energy Waste",
      "riskDescription": "Data centers and large AI models require substantial energy requirements for training and using large models with many billions of parameters. As such, deployment of AI can be harmful to the environment and impact an organization's sustainability goals.",
      "initialLikelihood": 2,
      "initialImpact": 2,
      "initialRiskLevel": 4,
      "riskLevelCategory": "Low",
      "exampleMitigations": "Choose Energy-Efficient AI: Enterprises should prioritize GenAI solutions that emphasize the energy efficiency of their algorithms. This includes models that require less energy for training and operation without compromising on performance. This can include techniques such as model pruning, or parameter quantization.\n\nChoose Sustainable Hardware: Choose either energy-efficient hardware, or data centers that run with renewable energy can significantly reduce environmental impact..\n\nPrefer Standard Solutions over AI: When all things are equal, prefer standard solutions over AI solutions, because they typically have a much better carbon-footprint.\n\nTraditional AI over GenAI: When all things are equal, prefer traditional AI solutions over GenAI solutions. Especially when standard AI solutions will be superior to GenAI solutions: e.g. do not attempt to use a GenAI solution for sales forecasting, or machine failure detection per se.",
      "agreedMitigation": "Address sustainability concerns by selecting cloud and AI vendors that demonstrate a commitment to energy efficiency and the use of renewable energy, and by incorporating these criteria into procurement and ESG reporting. Track the AI’s energy use and choose cloud providers with renewable energy. Optimize model runs (e.g., use smaller models or off-peak scheduling) to cut waste, and include AI energy metrics in sustainability reporting.\r\n\r\nTechnical: When selecting a cloud provider, review their public commitments and progress toward using renewable energy for their data centers. Use model compression or efficient architectures to reduce compute load. Procedural: Implement an internal \"green AI\" policy that requires evaluating the carbon impact for new AI projects. Incorporate energy efficiency and sustainability metrics into the AI and cloud services vendor selection process. Compliance: Include energy consumption metrics from AI usage in Dompé's annual corporate sustainability and ESG reports. Align with the company’s ISO 14001 environmental management practices by assessing AI’s carbon footprint and offsetting significant emissions.",
      "proposedOversightOwnership": "IT Leader, Ethics Board",
      "proposedSupport": "AI Leader",
      "notes": "This is a shared responsibility between AI and IT",
      "residualLikelihood": 2,
      "residualImpact": 1,
      "residualRiskLevel": 2
    },
    {
      "riskCategory": "Other Risks",
      "risk": "Workforce Obsolescence",
      "riskDescription": "The integration of AI and GenAI into business operations raises the risk of displacing human jobs, touching deeply on employee livelihoods and raising concerns among unions and worker groups about job security. The fear of job displacement is not unfounded, as history has shown that technological advancements can lead to shifts in the labor market, requiring workers to adapt to new roles or face unemployment. AI leaders should motivate their HR partners, if they are not already on it, to implement below mitigation strategies. Addressing HR obsolescence proactively allows organizations to mitigate AI's negative impacts on employment while leveraging these technologies for workforce enhancement and innovation. (see \"What To Do When Your Employees Fear Losing Their Jobs to GenAI\")",
      "initialLikelihood": 3,
      "initialImpact": 4,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Engagement & Communication: The organization should engage with unions, employee representatives, and other stakeholders in discussions about AI deployment. It shall keep employees informed about AI's role in the organization and support their transition to new roles. Enterprise-wide communication can help in identifying concerns, negotiating the transition, and hopefully facilitate that the adoption of AI technologies is socially responsible.\n\nReskilling and Upskilling: As an AI leader, you should work with your HR department. They start considering investing in training programs to equip employees with skills that complement AI, making the workforce adaptable and enhancing their ability to work with AI technologies. This may also mean to shift job roles towards tasks that require human skills such as creativity and empathy, ensuring employees focus on high-value activities.",
      "agreedMitigation": "Proactively manage workforce transition by framing the AI as a tool for augmentation, not replacement. Invest in upskilling programs so staff can move into more analytical or oversight roles that AI cannot fill, minimizing obsolescence. \r\n\r\nTechnical: Implement AI in a way that it handles tasks under human supervision, keeping employees in the loop; \r\n\r\nProcedural: Involve HR and employee representatives to manage change and offer retraining for new positions early. Develop and communicate a clear vision for how AI will augment human roles; partner with HR to create training programs focused on skills that complement AI, such as critical thinking, data analysis, and AI oversight.\r\n\r\nCompliance: adhere to labor regulations on job changes and be transparent with staff about AI deployment plans to build trust and manage the transition responsibly.",
      "proposedOversightOwnership": "HR, LOB",
      "proposedSupport": "AI Leaders",
      "notes": "This is primarily a responsibility LOB and HR.",
      "residualLikelihood": 3,
      "residualImpact": 1,
      "residualRiskLevel": 3
    },
    {
      "riskCategory": "Other Risks",
      "risk": "Workforce Dependency",
      "riskDescription": "The integration of AI in workplaces, while beneficial for efficiency and productivity, introduces a significant risk of dependency among professionals and a lack of a new workforce who developed the skills. This dependency risk emerges when employees become overly reliant on AI tools for tasks such as data analysis, decision-making, or even creative processes like writing and design. Over time, this reliance can lead to a degradation of professional skills, as individuals defer tasks to AI that they would previously have completed themselves, potentially leading to a skill atrophy in critical areas. Additionally, This dependence not only affects individual competency but also raises broader organizational risks. For example, if AI systems become unavailable due to technical issues, external restrictions, or ethical concerns, organizations might find their workforce ill-prepared to fill the gap, leading to operational vulnerabilities.",
      "initialLikelihood": 4,
      "initialImpact": 4,
      "initialRiskLevel": 16,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Balanced Integration of AI: Encourage a balanced approach to AI integration, where AI complements human skills rather than replaces them. Establish guidelines for when and how AI tools should be used, emphasizing AI as a support tool that enhances human work.\n\nEncourage Critical Thinking and Creativity: Design tasks and projects that require critical thinking, creativity, and problem-solving, areas where human judgment is paramount. This helps maintain and develop these essential skills within the workforce.\n\nDiversify Tools and Techniques: Avoid over-reliance on a single AI tool or system for tasks. Encourage the use of a variety of tools and methods to accomplish tasks, ensuring employees are versatile and not dependent on one technology.\n\nRegular Assessments and Feedback: Implement regular assessments of employee skills and provide feedback on performance, with specific attention to areas potentially affected by AI dependency. Use these assessments to identify areas for further training and development.\n\nEmployee Training and Communications Plan: Ensure employees understand not only how to use AI tools but also their limitations. Educating staff on the principles and boundaries of AI encourages more informed and discerning use of these technologies.",
      "agreedMitigation": "Maintain human expertise: require periodic manual performance of key tasks (e.g., manual data QC checks even if AI does them) and cross-train employees. Cultivate an environment where AI is a tool, not a crutch, by having employees double-check AI outputs. Foster a culture of critical engagement with AI by designing processes and training that emphasize AI as a support tool, not a replacement for human expertise, and by ensuring core manual skills are maintained.\r\n\r\nTechnical: Design the AI workflow to still require human input or final decision (so humans stay engaged). Procedural: Implement a balanced integration approach where AI is used for first drafts, but humans are required to perform critical analysis and editing; incorporate regular training sessions that include manual document analysis exercises to maintain skills Compliance: Include in SOPs that human review isn’t just for compliance but also to ensure knowledge retention, and have managers periodically test employees’ non-AI capabilities. Ensure  that the business continuity plan for system failure includes not just manual process steps but also refresher training resources",
      "proposedOversightOwnership": "LOBs, IT",
      "proposedSupport": "HR",
      "notes": "This is a shared responsibility between LOB, HR, and IT.",
      "residualLikelihood": 3,
      "residualImpact": 1,
      "residualRiskLevel": 3
    },
    {
      "riskCategory": "Other Risks",
      "risk": "Multiagency Drives Unseen Complexities",
      "riskDescription": "Multi-GenAI solutions will create unseen complexities and risks. An unintended consequence of GenAI is the collective emergent behaviors of such multi-agent GenAI solutions that may not be understood. We do not yet know all of the known risks and how we approach risk mitigation will need to evolve. At present, risk management focuses on the identification and mitigation of single AI models. The rise of complex sociotechnical systems of mixed AI, GenAI, human and standard IT solutions is less understood. Yet, we are only a few years away from where this likely becomes a reality.",
      "initialLikelihood": 2,
      "initialImpact": 4,
      "initialRiskLevel": 8,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "Consider educational programs or simulation tools designed for understanding and managing multi-agent GenAI systems\n\nDeploy increasingly advanced Monitoring and Control Mechanisms: Acknowledge complexities by offering real-time monitoring tools for detecting and responding to emergent behaviors and frameworks for dynamically adjusting the interactions between GenAI models based on observed outcomes. This can include tools that are part of the Gartner AI TRiSM framework",
      "agreedMitigation": "While not an immediate threat, address this future risk by establishing a strong AI governance framework now that mandates rigorous testing, monitoring, and validation for any future integration of multiple AI agentic systems.\r\n\r\nTechnical: Implement advanced monitoring tools to detect anomalous behaviors or outputs that could indicate negative emergent interactions between systems. Procedural: Any future project to integrate this GenAI system with another AI must undergo a specific, heightened risk assessment focusing on interaction risks. Compliance: Ensure Dompé's AI Governance Board is tasked with reviewing and approving any multi-agent AI architectures before deployment.",
      "proposedOversightOwnership": "AI Leader",
      "proposedSupport": "IT Leader",
      "notes": "This is a shared responsibility between IT and AI departments.",
      "residualLikelihood": 2,
      "residualImpact": 1,
      "residualRiskLevel": 2
    },
    {
      "riskCategory": "Other Risks",
      "risk": "Reputational Risk",
      "riskDescription": "Use of AI tools by employees or external consultants without governance and education can lead to misinterpretation of data, lack of verification and thus expose the company to reputational impact when assumption are based on biased or not accurate data.",
      "initialLikelihood": 4,
      "initialImpact": 5,
      "initialRiskLevel": 20,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Regular Assessments and Feedback: Implement regular assessments of employee skills and provide feedback on performance, with specific attention to area of external communication or generation of documents and material to be released. Use these assessments to identify areas for further training and development.\n\nEncourage Critical Thinking and Creativity: Design tasks and projects that require critical thinking, creativity, and problem-solving, areas where human judgment is paramount. This helps maintain and develop human oversight.\n\nEmployee Training and Communications Plan: Ensure employees understand not only how to use AI tools but also their limitations. Educating staff on the principles and boundaries of AI encourages more informed and discerning use of these technologies.",
      "agreedMitigation": "Protect corporate reputation by ensuring the AI system is used responsibly under strong governance, with mandatory human verification of all critical outputs and clear policies against unauthorized external sharing of AI-generated content. Be proactive: establish an AI ethics board to oversee uses, and have a crisis communication plan specifically for AI incidents. Monitor media and social sentiment regarding AI projects to respond quickly to negative attention.\r\n\r\nTechnical: Controls for Accuracy, Bias, and Data Leakage are the primary technical mitigations for reputational risk. Ensure AI undergoes rigorous pre-deployment testing to catch issues that could lead to public incidents. Procedural: Train the communications team on AI issues so they can explain and address them openly if needed. The HITL verification process is the key procedural control; additionally, the corporate communications policy must be updated to address handling AI-generated materials. Compliance: Adherence to all relevant regulations (GxP, GDPR, EU AI Act) is the foundation for maintaining a positive reputation. Voluntarily adhere to high standards (transparency, bias mitigation) beyond legal requirements to build a reputation as a responsible AI user, thereby insulating the brand against isolated errors.",
      "proposedOversightOwnership": "HR, LOB",
      "proposedSupport": "Communication",
      "notes": "This is a shared responsibility between LOB, HR.",
      "residualLikelihood": 2,
      "residualImpact": 3,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "Other Risks",
      "risk": "IP Risk",
      "riskDescription": "Ownership invention entirely generated by AI: GenAI and multi-agents integrations can authomatize the discovery of invention. If the human supervision and invention activity is not involved, there is a debate on the ownership of the invention which is not completely addressed in the IP field",
      "initialLikelihood": 3,
      "initialImpact": 4,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Human in the loop: Be sure that the process that may lead to invention includes human oversight and intervention, document human presence, by implementing different steps of revision, authorization and approval.\n\nHuman in the loop: Be sure that the process that may lead to invention includes human oversight and intervention, document human presence, by implementing different steps of revision, authorization and approval.\n\nHuman in the loop: Be sure that the process that may lead to invention includes human oversight and intervention, document human presence, by implementing different steps of revision, authorization and approval.",
      "agreedMitigation": "Mitigate IP risks by combining robust technical controls to prevent data leakage with a procedural framework that ensures and documents significant human intellectual contribution to any inventions derived from AI-assisted work. Clarify ownership in contracts: ensure outputs are legally assigned to Dompé and that vendors waive any claims. Consult IP counsel early for any AI-involved invention to strategize patentability.\r\n\r\nTechnical: Isolate AI environments for sensitive projects. The controls for Sensitive Information Leakage and Unauthorized Information Access are the primary technical mitigations for preventing trade secret disclosure.\r\n\r\nProcedural: Update internal policies so that any use of AI in R&D is reviewed by the IP legal team. Implement a \"human-in-the-loop\" process where any new ideas or potential inventions stemming from AI analysis are documented, with clear records of the human intellectual contribution required for patent filings\r\n\r\nCompliance: ensure NDAs and data processing agreements explicitly cover AI usage. 8090 will commit not to use Dompe inputs for other purposes, preserving trade secrets. Work with IP counsel to establish clear guidelines on inventorship for AI-assisted discoveries to ensure compliance with patent office requirements.",
      "proposedOversightOwnership": "LOB, IP",
      "proposedSupport": "Legal, Compliance",
      "notes": "This is a shared responsibility between LOB, IP",
      "residualLikelihood": 2,
      "residualImpact": 3,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "Other Risks",
      "risk": "GxP Risk",
      "riskDescription": "Regulation & GxP Impact: specific regulations governing the implementation of AI in GxP environment have not yet been fully established. The absence of predicate rules may expose organization to unpredctable interpretation by Health Authorities.\r\n\r\nRegulatory framework may evolve over time, potetially introducing non-compliance risks after development and deployment",
      "initialLikelihood": 4,
      "initialImpact": 4,
      "initialRiskLevel": 16,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Assess the AI solution to be adopted in GxP environment to identify risks and impact on data and processes.\n\nCreate an internal quality system governing the implementation of AI solution and stricty monitor the evolution of the regulation.\n\n- prioritize the use of interpretable machine learning models over more opaque generative AI solution, particularly in critical GxP applications\r\n- ensure continous human oversight of both data generated by the AI system and the processes used to produce them\r\n- define robust data and model quality metrics\r\n- provide training to staff involved in AI use or supervision, to ensure understanding of risks, limitations and responsibilities associated with AI implementation\n\nHuman oversight not in the loop: define a validation strategy\r\n\r\nHuman oversight in the loop: assess the AI solution in reference to the potential impact on GxP relevant data\n\nProduction Environment: define a validation/verification strategy\n\nDefine an action plan aimed at:\r\n- preventing unauthorized deletion or alteration of data\r\n- ensuring compliance with ALCOA+ principles\r\n- controlling access, ensuring audit trails and defining roles and responsibilities",
      "agreedMitigation": "Perform a risk assessment for the adoption of each AI solution: the assessment is based upon the indication of the intended use as indicated by the process owner, the analysis of eventual interfaces with validated platforms, the presence of human oversight in the loop. Based on this, evaluation of direct/indirect impact on product quality and/or patient safety is conducted and the relevant mitigation actions defined accordingly. ",
      "proposedOversightOwnership": "Quality, Process Owner",
      "proposedSupport": "AI Leader, IT",
      "notes": "",
      "residualLikelihood": 3,
      "residualImpact": 2,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "Business/Cost Related Risks",
      "risk": "Vendor Lock-in",
      "riskDescription": "Vendor Lock-in occurs when a company becomes overly dependent on a single AI or third-party provider, making it difficult to switch to alternative solutions. This can lead to reduced bargaining power, limited flexibility, and potential disruptions if the provider changes terms or discontinues services.",
      "initialLikelihood": 4,
      "initialImpact": 3,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Negotiate exit clauses and data portability at the outset of any vendor agreement to ensure flexibility in transitioning away from a AI provider if needed.\r\n\n\nMaintain dual-vendor strategies where feasible to reduce dependency on a single provider and ensure continuity in case of service disruption.\n\nPeriodically assess market alternatives to stay informed about emerging solutions and maintain leverage in vendor negotiations.",
      "agreedMitigation": "Design the system with a modular architecture that abstracts the core business logic from the specific cloud vendor's AI model. Develop the application using a standardized API interface for the GenAI model with frameworks like LiteLLM or OpenRouter, allowing for the potential to swap out the underlying model from a different vendor with less engineering effort. Periodically assess market alternatives to stay informed about emerging solutions. The contract must include clauses for data portability, termination rights, and transition assistance to ensure a viable exit path. \r\n\r\n**A. Technical Controls (Provided by 8090):**\r\n1. **Data Portability:** 8090 will ensure that Dompe can export all its proprietary data (e.g., prompt templates, usage logs) in a standard, machine-readable format (e.g., JSON, CSV).\r\n2. **Open Standards:** The system will prioritize using open standards for integration and interoperability where feasible.",
      "proposedOversightOwnership": "IT, Legal",
      "proposedSupport": "AI Leader",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 1,
      "residualRiskLevel": 2
    },
    {
      "riskCategory": "Business/Cost Related Risks",
      "risk": "Service Discontinuity",
      "riskDescription": "Service Discontinuity refers to the risk of sudden termination or degradation of services provided by AI or third-party vendors, especialy in case of new models beeing pushed out by the provider. This can result in operational disruptions, loss of critical functionalities, and negative impacts on business continuity.",
      "initialLikelihood": 3,
      "initialImpact": 4,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Include SLAs and continuity clauses in contracts, and establish contingency plans and backup providers to safeguard against unexpected service interruptions, especialy for process-integrated models.",
      "agreedMitigation": "Mitigate the risk of model deprecation by maintaining a proactive model upgrade lifecycle management plan for the AI system. Develop a contingency plan outlining the steps for migrating to and revalidating a new model version, including resource allocation, improvement engineering, and timelines. The vendor/hyperscaler contract must include clauses that guarantee a minimum notification period (e.g., 12 months) before a model version is deprecated and ensure access to the new model for validation. Regularly export and backup any data from the AI platform to ensure we have what we need if the service ends. Include service discontinuity scenarios in business continuity planning and validate that plan with a tabletop exercise simulating a vendor/model outage.",
      "proposedOversightOwnership": "IT Security",
      "proposedSupport": "Risk Management, Legal; Compliance",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "Business/Cost Related Risks",
      "risk": "Lack of Transparency",
      "riskDescription": "Lack of Transparency involves limited visibility into how AI models are trained, how decisions are made, and how data is processed. This can hinder the company's ability to understand, trust, and validate the outputs, leading to potential compliance and ethical issues.This is especialy relevant in datapools and web-research basin of the model, which may end up beeing decisive in the quality and veridicity of the autput.",
      "initialLikelihood": 3,
      "initialImpact": 3,
      "initialRiskLevel": 9,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "Request SandBox Environments, whenever available, in order to test the deicionmaking process and quality of output of AI models extensively, before deploying them in company processes.\n\nUse explainability tools and third-party source validation, whenever possible, in order  to verify the integrity and interpretability of AI outputs, especially in regulated environments.\n\nInclude transparency clauses in vendor agreements to formalize expectations around disclosure, traceability, and auditability of AI systems.",
      "agreedMitigation": "Address the core model's opacity by focusing controls on the transparent elements: the input data, the verified output, and the vendor's contractual obligations and certifications. Supplement with Dompe's validation: thoroughly test the AI and document its performance characteristics since we can’t see inside it. Maintain open communication with 8090 for any critical questions.",
      "proposedOversightOwnership": "Compliance, AI Leader",
      "proposedSupport": "Legal, Audit",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 1,
      "residualRiskLevel": 2
    },
    {
      "riskCategory": "Business/Cost Related Risks",
      "risk": "Limited Customization",
      "riskDescription": "Limited Customization refers to the inability to tailor AI solutions to specific business needs. This can result in suboptimal performance, misalignment with business objectives, and reduced competitive advantage.",
      "initialLikelihood": 3,
      "initialImpact": 3,
      "initialRiskLevel": 9,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "Prioritize modular and Application Programming Interface (API) solutions to enable easier integration, customization, and future scalability of AI tools.\n\nCollaborate with vendors on roadmap alignment to ensure that product development supports evolving business and compliance needs.\n\nMaintain internal capability for critical adaptations to avoid over-reliance on external vendors for essential system modifications.",
      "agreedMitigation": "Focus customization efforts on the areas Dompé controls: the quality and structure of the data corpus and the development of sophisticated, task-specific prompt engineering. Invest in developing a library or a custom knowledge base of advanced, validated prompt templates tailored for different document types (e.g., CSR, PSUR) and user queries to maximize performance from the base model. Establish a continuous feedback loop where users can report suboptimal responses, feeding into a process for refining the data and prompt templates. Ensure any inability to customize that affects a GxP process is documented and mitigated, for example, by manual steps to fill the gap.",
      "proposedOversightOwnership": "IT Security, AI Leader",
      "proposedSupport": "LOBs, Procurement",
      "notes": "",
      "residualLikelihood": 3,
      "residualImpact": 2,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "Business/Cost Related Risks",
      "risk": "Vendor Compliance Misalignment",
      "riskDescription": "Compliance Misalignment occurs when the practices of AI or third-party providers do not align with evolving regulatory requirements. This can lead to legal and financial penalties, reputational damage, and increased scrutiny from regulatory bodies.",
      "initialLikelihood": 4,
      "initialImpact": 5,
      "initialRiskLevel": 20,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Conduct regular compliance test/audits to verify that AI vendors adhere to internal policies and external regulatory requirements.\n\nRequire adherence to your internal Code of Conduct to ensure that vendor practices align with your organization’s ethical and operational standards.\n\nInclude regulatory alignment clauses in contracts to hold vendors accountable for compliance with applicable laws and industry standards.",
      "agreedMitigation": "Mitigate vendor compliance risk through initial due diligence, periodic monitoring, and contractual obligations that require the vendor to maintain compliance with all applicable laws and regulations. Conduct regular compliance audits of the vendor, including reviewing their certifications (SOC 2, ISO 27001) and policies. The vendor contract must include clauses that require adherence to all applicable laws and provide Dompé with the right to audit the vendor for compliance.",
      "proposedOversightOwnership": "Legal, Compliance",
      "proposedSupport": "Procurement, Privacy",
      "notes": "",
      "residualLikelihood": 3,
      "residualImpact": 3,
      "residualRiskLevel": 9
    },
    {
      "riskCategory": "Business/Cost Related Risks",
      "risk": "Lifecycle Costs",
      "riskDescription": "Increasing lifecycle Costs due to vendor lock-in can result in escalating fees, unfavorable contract terms, and higher total cost of ownership. This can strain the company's budget, reduce profitability, and limit resources for other strategic initiatives.",
      "initialLikelihood": 4,
      "initialImpact": 3,
      "initialRiskLevel": 12,
      "riskLevelCategory": "High",
      "exampleMitigations": "Perform Total Cost of Ownership (TCO) analysis to evaluate the long-term financial impact of AI solutions beyond initial implementation costs.\n\nBenchmark pricing and renegotiate periodically to ensure that contract terms remain competitive and reflect market conditions.\n\nTrack hidden costs such as integration and retraining to maintain visibility over the full financial footprint of AI deployment.",
      "agreedMitigation": "Manage lifecycle costs through detailed financial modeling, monitoring of usage and spending, and negotiating predictable, scalable pricing models. Implement robust monitoring and alerting on cloud and AI service consumption. Use tagging to attribute costs to specific business units or projects. Perform a Total Cost of Ownership (TCO) analysis, including licensing and data management costs, variable inference costs, human verification, and re-validation. Negotiate pricing terms that are transparent and scalable.",
      "proposedOversightOwnership": "Finance, IT Security",
      "proposedSupport": "AI Leader",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 1,
      "residualRiskLevel": 2
    },
    {
      "riskCategory": "Business/Cost Related Risks",
      "risk": "Sensitive Information Access",
      "riskDescription": "Sensitive Information Access involves the risk that AI or third-party providers may access, mishandle, or leak sensitive or intellectual property data. This can lead to data breaches, loss of competitive advantage, and legal repercussions.",
      "initialLikelihood": 4,
      "initialImpact": 4,
      "initialRiskLevel": 16,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Enforce strict data access controls and encryption to protect sensitive information from unauthorized access or misuse, also depicted in a strengthened Data Governance.\n\nUse privacy-enhancing technologies to minimize data exposure and support compliance with data protection regulations.\n\nConduct vendor risk assessments and Data Protection Assessment DPA reviews to evaluate how third parties handle sensitive data and ensure contractual safeguards are in place.",
      "agreedMitigation": "Prevent unauthorized vendor access to sensitive information by using a combination of strong encryption with customer-managed keys and strict contractual agreements. Use enterprise-grade encryption standards for document stores and databases. Use synthetic or anonymized data sets to avoid using real personal data. The vendor contract and BAA/DPA must explicitly state the controls the vendor is exercising when accessing PII/PHI content for any purpose, and detail the security controls, like encryption, that enforce this separation.",
      "proposedOversightOwnership": "Privacy, IT Security",
      "proposedSupport": "Compliance",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 3,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "AI Human Impact Risks",
      "risk": "Algorithmic Misjudgment",
      "riskDescription": "AI-generated evaluations (e.g., performance, compliance, productivity) may be inaccurate or biased, leading to unfair treatment or HR decisions.",
      "initialLikelihood": 4,
      "initialImpact": 5,
      "initialRiskLevel": 20,
      "riskLevelCategory": "Critical",
      "exampleMitigations": "Implement human-in-the-loop review to ensure that any AI-driven decision-making process includes documented human oversight, revision, and approval.\r\n\n\nConduct bias test/audits to identify and mitigate potential discriminatory patterns in AI outputs, especially in HR or compliance contexts.",
      "agreedMitigation": "Mandatory Human Oversight and Accountability: 1. Human-in-the-Loop as a GxP Control: Formalize the HITL review process within a Standard Operating Procedure (SOP). The SOP must define that no GxP decision can be made based solely on the AI's output. The AI is a tool for assistance; the human user remains the accountable decision-maker. 2. Documented Verification: The human verification step must be explicitly documented. For critical GxP records, this verification must be captured via a 21 CFR Part 11-compliant electronic signature, creating an auditable record of human oversight.10 3. Training on AI Limitations: All users must complete mandatory training that covers the inherent limitations of GenAI, including the risks of hallucinations, bias, and the importance of critical thinking and independent verification of AI outputs. Training records must be maintained per GxP requirements.1",
      "proposedOversightOwnership": "HR, Compliance",
      "proposedSupport": "AI Leader, Legal",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 3,
      "residualRiskLevel": 6
    },
    {
      "riskCategory": "AI Human Impact Risks",
      "risk": "Opaque Decision-Making",
      "riskDescription": "Employees may not understand how AI reached a conclusion, undermining trust and accountability.",
      "initialLikelihood": 3,
      "initialImpact": 3,
      "initialRiskLevel": 9,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "Deploy explainability tools to help users and stakeholders understand how AI systems reach their conclusions.\n\nEstablish disclosure policies to inform individuals when AI is involved in decision-making processes that affect them.\n\nProvide an appeals process to allow individuals to challenge or request clarification on AI-generated outcomes.",
      "agreedMitigation": "Address user-level concerns about opacity by providing transparency where it matters most: clearly citing the exact source of the information used, thereby building trust in the output's grounding even if the synthesis process is a black box.Technical: Every AI-generated answer must be accompanied by a clear, clickable citation linking directly to the source document, page, and section used for the response. Procedural: User training must educate employees on the system's capabilities and limitations, explaining that it is a summarization tool grounded in specific documents and encouraging them to use the citations to verify information. Compliance: This approach aligns with the principle of transparency promoted by the EU AI Act and GDPR",
      "proposedOversightOwnership": "Compliance, HR",
      "proposedSupport": "IT, Audit",
      "notes": "",
      "residualLikelihood": 2,
      "residualImpact": 2,
      "residualRiskLevel": 4
    },
    {
      "riskCategory": "AI Human Impact Risks",
      "risk": "Psychological Harm",
      "riskDescription": "Negative feedback or performance scoring from AI may cause stress, demotivation, or reputational damage.",
      "initialLikelihood": 2,
      "initialImpact": 3,
      "initialRiskLevel": 6,
      "riskLevelCategory": "Medium",
      "exampleMitigations": "Limit AI to advisory roles in sensitive contexts such as performance evaluation or compliance monitoring to preserve human accountability.\n\nTrain managers on ethical use of AI to ensure responsible deployment and interpretation of AI outputs.\n\nProvide opt-out options where appropriate to respect individual autonomy and reduce reliance on automated assessments.",
      "agreedMitigation": "The risk is negligible as the system's intended use is not related to employee performance evaluation; therefore, the risk is accepted without specific controls beyond the defined scope of use.The system's documented intended use, as defined in the validation plan and user training, explicitly excludes any form of employee performance monitoring or evaluation",
      "proposedOversightOwnership": "HR, Compliance",
      "proposedSupport": "Ethics Board, LOB",
      "notes": "",
      "residualLikelihood": 1,
      "residualImpact": 1,
      "residualRiskLevel": 1
    },
    {
      "riskCategory": "AI Human Impact Risks",
      "risk": "Discrimination Risk",
      "riskDescription": "AI may replicate or amplify bias in hiring, promotion, or disciplinary actions.",
      "initialLikelihood": 1,
      "initialImpact": 1,
      "initialRiskLevel": 1,
      "riskLevelCategory": "Low",
      "exampleMitigations": "Conduct fairness testing to evaluate AI systems for equitable treatment across different user groups.\n\nUse diverse training data to reduce the risk of systemic bias in AI outputs.",
      "agreedMitigation": "The risk is negligible as the system is not intended for use in any HR or employment-related decision-making processes; the risk is accepted, and its use is prohibited for such purposes in the AUP.  The system's Acceptable Use Policy (AUP) will explicitly prohibit its use for any purpose related to hiring, promotion, performance management, or other HR decisions. This prohibition aligns with the EU AI Act's classification of employment-related AI as high-risk, a category Dompé is actively avoiding for this tool.",
      "proposedOversightOwnership": "HR, Legal",
      "proposedSupport": "Compliance",
      "notes": "",
      "residualLikelihood": 1,
      "residualImpact": 1,
      "residualRiskLevel": 1
    },
    {
      "riskCategory": "AI Human Impact Risks",
      "risk": "Consent & Transparency Gaps",
      "riskDescription": "Employees may be unaware they are being evaluated by AI or how their data is used.",
      "initialLikelihood": 2,
      "initialImpact": 2,
      "initialRiskLevel": 4,
      "riskLevelCategory": "Low",
      "exampleMitigations": "Secure informed consent from individuals when AI is used in processes that affect them directly.\n\nProvide clear documentation to explain the role and limitations of AI in decision-making.\n\nRun internal awareness campaigns to educate employees about AI use, their rights, and available safeguards.",
      "agreedMitigation": "Ensure full transparency with employees by providing clear communication and training on how the system works, what data (including their queries) is logged, and for what purpose (e.g., security auditing, system improvement). User training materials and the AUP must clearly explain that all interactions with the system are logged for security and operational purposes. This transparency aligns with the general principles of data protection under GDPR, even for employee data, and the transparency focus of the EU AI Act.",
      "proposedOversightOwnership": "Compliance, Privacy",
      "proposedSupport": "HR, IT",
      "notes": "",
      "residualLikelihood": 1,
      "residualImpact": 1,
      "residualRiskLevel": 1
    }
  ],
  "controlsMapping": [
    {
      "mitigationID": "ACC-01",
      "mitigationDescription": "Human-in-the-Loop (HITL) Verification with 21 CFR 11-compliant e-signature for GxP decisions.",
      "cfrPart11Annex11": "11.10(a), 11.50, 11.100; Annex 11: 1, 15",
      "hipaaSafeguard": "§ 164.308(a)(1)(i) (Security Management Process)",
      "gdprArticle": "Art. 22 (Right to human intervention)",
      "euAiActArticle": "Art. 14 (Human Oversight)",
      "nist80053": "IA-2, IA-4, IA-5, CM-3",
      "soc2TSC": "PI 1.2, PI 1.4",
      "category": "Accuracy & Judgment"
    },
    {
      "mitigationID": "ACC-02",
      "mitigationDescription": "Training on AI limitations, critical thinking, and independent verification.",
      "cfrPart11Annex11": "11.10(i); Annex 11: 2",
      "hipaaSafeguard": "§ 164.308(a)(5) (Security Awareness)",
      "gdprArticle": "Art. 32 (Security of processing)",
      "euAiActArticle": "Art. 14(5) (Training of users)",
      "nist80053": "AT-2, AT-3",
      "soc2TSC": "CC1.2, CC2.2",
      "category": "Accuracy & Judgment"
    },
    {
      "mitigationID": "ACC-03",
      "mitigationDescription": "Implement a Retrieval-Augmented Generation (RAG) architecture grounded in a validated, version-controlled corpus.",
      "cfrPart11Annex11": "11.10(a); Annex 11: 4 (Validation)",
      "hipaaSafeguard": "§ 164.310(d)(1) (Data Integrity)",
      "gdprArticle": "Art. 25 (Data protection by design)",
      "euAiActArticle": "Art. 10 (Data and data governance)",
      "nist80053": "SI-7, CM-2, SA-8",
      "soc2TSC": "PI 1.1, A1.1",
      "category": "Accuracy & Judgment"
    },
    {
      "mitigationID": "SEC-01",
      "mitigationDescription": "Implement infrastructure isolation with VPC Service Controls and a Web Application Firewall (WAF).",
      "cfrPart11Annex11": "11.10(d); Annex 11: 12 (Security)",
      "hipaaSafeguard": "§ 164.312(c)(1) (Access Control)",
      "gdprArticle": "Art. 32 (Security of processing)",
      "euAiActArticle": "Art. 15 (Robustness and accuracy)",
      "nist80053": "SC-7, AC-4, CA-3",
      "soc2TSC": "CC6.6, CC7.1",
      "category": "Security & Data Privacy"
    },
    {
      "mitigationID": "SEC-02",
      "mitigationDescription": "Enforce strict, role-based access controls (RBAC) based on the principle of least privilege.",
      "cfrPart11Annex11": "11.10(d), 11.10(g); Annex 11: 12",
      "hipaaSafeguard": "§ 164.312(a)(1) (Access Control)",
      "gdprArticle": "Art. 32(1)(b)",
      "euAiActArticle": "Art. 14 (Human Oversight)",
      "nist80053": "AC-2, AC-3, AC-6",
      "soc2TSC": "CC6.1, CC6.3",
      "category": "Security & Data Privacy"
    },
    {
      "mitigationID": "SEC-03",
      "mitigationDescription": "Encrypt all data at rest and in transit using  encryption keys.",
      "cfrPart11Annex11": "11.30; Annex 11: 12.4",
      "hipaaSafeguard": "§ 164.312(a)(2)(iv), § 164.312(e)(1)",
      "gdprArticle": "Art. 32(1)(a)",
      "euAiActArticle": "Art. 15 (Cybersecurity)",
      "nist80053": "SC-8, SC-12, SC-13, SC-28",
      "soc2TSC": "C1.1, CC6.1",
      "category": "Security & Data Privacy"
    },
    {
      "mitigationID": "SEC-04",
      "mitigationDescription": "Implement data loss prevention (DLP) to scan and redact sensitive data in prompts/responses.",
      "cfrPart11Annex11": "11.10(c); Annex 11: 5 (Data)",
      "hipaaSafeguard": "§ 164.312(c)(1)",
      "gdprArticle": "Art. 25 (Data protection by design)",
      "euAiActArticle": "Art. 10(5) (Data governance)",
      "nist80053": "SI-4, SC-4",
      "soc2TSC": "C1.1, P1.1",
      "category": "Security & Data Privacy"
    },
    {
      "mitigationID": "SEC-05",
      "mitigationDescription": "Conduct periodic adversarial testing Red Teaming of the GenAI system.",
      "cfrPart11Annex11": "11.10(a); Annex 11: 1 (Risk Management)",
      "hipaaSafeguard": "§ 164.308(a)(1)(ii)(A) (Risk Analysis)",
      "gdprArticle": "Art. 32(1)(d) (Testing)",
      "euAiActArticle": "Art. 15 (Robustness), Art. 9(6) (Testing)",
      "nist80053": "RA-5, CA-8",
      "soc2TSC": "CC7.1",
      "category": "Security & Data Privacy"
    },
    {
      "mitigationID": "LOG-01",
      "mitigationDescription": "Implement centralized, immutable logging of all user prompts, system responses, and administrative actions.",
      "cfrPart11Annex11": "11.10(e); Annex 11: 9 (Audit Trails)",
      "hipaaSafeguard": "§ 164.312(b) (Audit Controls)",
      "gdprArticle": "Art. 32",
      "euAiActArticle": "Art. 12 (Logs)",
      "nist80053": "AU-2, AU-9, AU-11",
      "soc2TSC": "CC7.1, CC7.2",
      "category": "Audit & Traceability"
    },
    {
      "mitigationID": "LOG-02",
      "mitigationDescription": "Ensure all AI-generated outputs include precise citations to source GxP documents.",
      "cfrPart11Annex11": "11.10(b); Annex 11: 8 (Printouts)",
      "hipaaSafeguard": "N/A",
      "gdprArticle": "Art. 15 (Right of access)",
      "euAiActArticle": "Art. 13 (Transparency)",
      "nist80053": "N/A",
      "soc2TSC": "PI 1.1",
      "category": "Audit & Traceability"
    },
    {
      "mitigationID": "GOV-01",
      "mitigationDescription": "Establish a formal AI Governance Committee and a continuous regulatory intelligence program.",
      "cfrPart11Annex11": "Annex 11: 1, 2",
      "hipaaSafeguard": "§ 164.308(a)(2) (Assigned Security Responsibility)",
      "gdprArticle": "Art. 24 (Responsibility of the controller)",
      "euAiActArticle": "Art. 9 (Risk management system)",
      "nist80053": "PM-9, RA-1, PL-2",
      "soc2TSC": "CC1.1, CC3.1",
      "category": "Governance & Compliance"
    },
    {
      "mitigationID": "GOV-02",
      "mitigationDescription": "Enforce vendor contracts with IP indemnification and prohibitions on using Dompé data for training.",
      "cfrPart11Annex11": "Annex 11: 3 (Suppliers)",
      "hipaaSafeguard": "§ 164.308(b)(1) (Business Associate Contracts)",
      "gdprArticle": "Art. 28 (Processor)",
      "euAiActArticle": "Art. 28 (Obligations of deployers)",
      "nist80053": "SA-9, SA-12",
      "soc2TSC": "CC9.2",
      "category": "Governance & Compliance"
    },
    {
      "mitigationID": "GOV-03",
      "mitigationDescription": "Implement a Continuous Validation framework integrated into the MLOps pipeline.",
      "cfrPart11Annex11": "11.10(a); Annex 11: 4",
      "hipaaSafeguard": "§ 164.308(a)(8) (Evaluation)",
      "gdprArticle": "Art. 32(1)(d)",
      "euAiActArticle": "Art. 9(2) (Continuous iterative process)",
      "nist80053": "CM-3, CM-4, SA-11, RA-5",
      "soc2TSC": "CC8.1, CC4.1",
      "category": "Governance & Compliance"
    }
  ],
  "riskCategories": [
    "Behavioral Risks",
    "Transparency Risks",
    "Security and Data Risks",
    "Other Risks",
    "Business/Cost Related Risks",
    "AI Human Impact Risks"
  ],
  "scoringMethodology": "🧮 Risk Scoring Methodology\r\nEach risk is evaluated using a standard risk assessment model:\r\n\r\nLikelihood (1–5): Estimates how probable the risk is to occur.\r\n\r\n1 = Rare\r\n2 = Unlikely\r\n3 = Possible\r\n4 = Likely\r\n5 = Almost Certain\r\nImpact (1–5): Measures the severity of consequences if the risk materializes.\r\n\r\n1 = Negligible\r\n2 = Minor\r\n3 = Moderate\r\n4 = Major\r\n5 = Catastrophic\r\nRisk Level = Likelihood × Impact\r\n\r\nRisk Level Category:\r\n\r\n1–5: Low\r\n6–10: Medium\r\n11–15: High\r\n16–25: Critical"
}